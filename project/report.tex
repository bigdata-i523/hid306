\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
		
	\title{Predicting Housing Prices - Kaggle Competition}	
	
	\author{Murali Cheruvu, Anand Sriramulu}
	\orcid{xxxx-xxxx-xxxx}
	\affiliation{%
		\institution{Indiana University}
		\streetaddress{3209 E 10th St}
		\city{Bloomington} 
		\state{Indiana} 
		\postcode{47408}
	}
	\email{mcheruvu@iu.edu, asriram@iu.edu}
	
	% The default list of authors is too long for headers}
	\renewcommand{\shortauthors}{M. Cheruvu, A Sriramulu}
	
	
	\begin{abstract}
		
	Apply exploratory data analysis and implement various advanced supervised machine learning algorithms to predict neighborhood housing sale prices found in the sample test dataset. Compare the predicted models and results from these advanced supervised algorithms. Apply ensembled model to achieve better predictions, hence get good score in kaggle competition.
	
	\end{abstract}
	
	\keywords{i523, hid306, Supervised Learning Algorithms, Exploratory Data Analysis, Kaggle}
	
	\maketitle
	
	\section{Background}
	
	Kaggle website hosts a platform for data science competitions. Predicting housing prices is a competition project, run and maintain by Kaggle. This project has two data sets - {\em train} and {\em test}, with 79 exploratory variables describing almost every aspect of residential homes in city of Ames, Iowa state. The goal of this competition is to predict the sale prices of residential homes listed in the test dataset as accurately as possible. Submissions are evaluated on {\em root mean squared error} (RMSE) between the logarithm of predicted value and the logarithm of observed sale price, also known as {\em kaggle score}. Submitted file expects two variables: row id and the predicted sale price of each home listed in the test dataset. Our goal is to participate in this competition and get placed within top 15\% of the submitted algorithms.
	
	
	\section{Introduction} 
	
	Training dataset of the {\em predicting housing sale prices} project contains sale price of the homes, and using this training data set, how accurately we can predict sale prices  of the homes in the test dataset using preprocessing and thorough data analysis. Many developers used advanced learning algorithms - XGBoost, Lasso and Neural Network, to predict the sale prices in the competition and achieved better kaggle scores. We have applied various exploratory analysis techniques and engineer the features before applying a few advanced supervised learning algorithms to create more accurately predicted models. 
	
		
	\section{Exploratory Data Analysis} 
		
	There are 1460 rows in the training data set and 1459 rows in the test dataset. Out of the 80 variables, 23 are nominal, 23 are ordinal, 14 are discrete, and 20 are continuous. We have combined training and testing datasets for easier analysis. We excluded Id attribute as it does not add value in the modeling. We also removed Sale Price, the target variable, from the training dataset. All the variables are listed in the appendix section as a reference.
	
	\subsection{Handling Missing Values}
	
	First part of the analysis was to check for any missing values in the training and testing datasets as shown in figure (\ref{c:check-nulls}). Using the bar plot shown in figure (\ref{fig:missing-values}), we have identified that there are 5 variables: {\em pool quality}, {\em miscellaneous features}, {\em alley}, {\em fence} and {\em fire place quality}, having the most missing data. All the missed values for the numeric variables are analyzed further to decide whether we need to delete the instances of all the data with missing values or fill them using meaningful data such as median of the corresponding variable. 
	
	\begin{figure}[htb]
	\begin{verbatim}		
	# python code - check for null values
	train = pd.read_csv('../data/train.csv')
	test = pd.read_csv('../data/test.csv')
		
	#combine the data sets
	alldata = train.append(test)	
	na = alldata.isnull().sum()
	           .sort_values(ascending=False)
	\end{verbatim}
	\caption{Code - Null Checks}\label{c:check-nulls}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.9\columnwidth]{images/missing_values}	
		\caption{Graph - Missing Values} \label{fig:missing-values} 
	\end{figure}

	\subsection{Analyze Numerical Variables}
	There are 37 numerical variables after excluding the {\em Id} variable. We have analyzed all the numerical variables for data patterns such as skewness in the data and range of the possible values. We have shown {\em sale price}, {\em overall quality}, {\em garage live area} and {\em year built}, in the figures (\ref{fig:num-feature-1}) and (\ref{fig:num_features_2}) as a few sample plots from the numerical analysis. Corresponding code snippet is shown in figure (\ref{c:analyze-numeric}).
	
	\begin{figure}[htb]
	\begin{verbatim}
	# python code - analyze numeric variables
	numerical_features = [f for f in train.columns 
						if train.dtypes[f] != 'object']
	
	nd = pd.melt(train, value_vars = numerical_features)	
	plt.figure(figsize = (5,3))
	plot = sns.FacetGrid (nd, col='variable', col_wrap=4,
	                 sharex=False, sharey = False)
	plot = plot.map(sns.distplot, 'value')				
	\end{verbatim}
	\caption{Code - Numerical Analysis}\label{c:analyze-numeric}
	\end{figure}

	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.75\columnwidth]{images/num_features_1}	
		\caption{Graph - Sale Price and Overall Quality}\label{fig:num-feature-1}		
	\end{figure}

	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.75\columnwidth]{images/num_features_2}	
		\caption{Graph - Ground Live Area and Year Built} \label{fig:num_features_2} 
	\end{figure}

	\subsection{Analyze Categorical Variables}
	There are 43 categorical variables in the combined dataset. We have analyzed all categorical variables and found the ways to fill the missing values. We has also evaluated proper approaches to convert them into numerical factors. Later on in the feature engineering section, we will go through more details on numerical factors. Categorical variable factors and the corresponding code snippet for {\em neighborhood} and {\em sale type} are shown in figure (\ref{c:analyze-cat}) and figures (\ref{fig:cat_features_1}).
	
	\begin{figure}[htb]
	\begin{verbatim}	
	# python code - analyze numeric variables
	cat_features = [f for f in train.columns 
			if train.dtypes[f] == 'object']
	print(cat_features)
	
	def barplot(x,y,**kwargs):
	    sns.barplot(x=x,y=y)
	    x = plt.xticks(rotation=90)
	
	plt.figure(figsize = (5,3))
	
	p = pd.melt(train, id_vars='SalePrice',
	             value_vars=cat_features)
	             
	g = sns.FacetGrid (p, col='variable', col_wrap=4, 
	sharex=False, sharey=False, size=5)
	
	g = g.map(barplot, 'value','SalePrice')				
	\end{verbatim}
	\caption{Code - Categorical Analysis} \label{c:analyze-cat} 
	\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/cat_features_1}	
	\caption{Graph - Neighborhood and Sale Type} \label{fig:cat_features_1} 
\end{figure}
	
	\subsection{Analyze Correlations}		
	{\em Numpy} package offers correlations functionality to analyze the variables that are positively or negatively correlated with the {\em sale price} and also analyze any interdependencies among the variables. Figure (\ref{c:cor}) and (\ref{fig:correlations}) shows the code snippet and the correlations plot. From that we can list the top 10 features those are strongly correlated with the target variable - {\em sale price}. We can visualize a few pair-wise correlation graphs with sale price for further detailed analysis. Figures (\ref{fig:pair-wise-correlations}) and (\ref{fig:pair-wise-correlations-2}) show how {\em overall quality}, {\em ground live area}, {\em garage cars} and {\em garage area} are positively correlated with {\em sale price}.
	
	\begin{figure}[htb]
	\begin{verbatim}	
	# python code 
	corr = alldata[numerical_features].corr()	
	mask = np.zeros_like(corr)
	mask[np.triu_indices_from(mask)] = True	
	plt.figure(figsize = (15,8))	
	sns_plot = sns.heatmap(corr, cmap='YlGnBu', 
	               linewidths=.5,  mask=mask, vmax=.3)	
	\end{verbatim}
	\caption{Code - Correlations} \label{c:cor} 
	\end{figure}

	\begin{figure}[htb]
		\centering
		\includegraphics[width=1.2\columnwidth]{images/correlations}	
		\caption{Graph - Correlations with Sale Price} \label{fig:correlations} 
	\end{figure}
		
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.9\columnwidth]{images/pair_wise_correlations_1}	
		\caption{Graph - Overall Quality and Ground Live Area} \label{fig:pair-wise-correlations} 
	\end{figure}

	\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\columnwidth]{images/pair_wise_correlations_2}	
	\caption{Graph - Garage Cars and Garage Area} \label{fig:pair-wise-correlations-2} 
	\end{figure}

	\begin{enumerate}
		\item OverallQual: Overall material and finish quality
		\item GrLivArea: Above ground living area square feet
		\item GarageCars: Size of garage in car capacity
		\item GarageArea: Size of garage in square feet
		\item TotalBsmtSF: Total square feet of basement area
		\item 1stFlrSF: First Floor square feet
		\item FullBath: Full bathrooms above grade
		\item TotRmsAbvGrd: Total rooms above ground
		\item YearBuilt: Original construction date
		\item GarageYrBlt: Garage built year		
	\end{enumerate}

	\subsection{Skewed Data Analysis}
	From the numerical analysis, we have identified that there are a few numerical variables need further analysis to identify the skewed data. We did not find any key variables those have skewed more than 75\%. However, we wanted to replace the {\em sale price} with corresponding logarithmic value for the predictive models and later convert it back to the exponential value before submitting to the kaggle competition. Figure (\ref{fig:sale-price-skew}) shows the {\em sale price}, before and after applying the logarithmic value. 
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=1.0\columnwidth]{images/sale_price_skew}	
		\caption{Graph - Sale Price skewness} \label{fig:sale-price-skew} 
	\end{figure}

	\subsection{Outlier Analysis}
	
	Continuing with exploratory analysis, we have analyzed the outliers using {\em Cook's distance}. Cook's distance is a measure calculated from a regression model to find out the influence exerted by each observation (row) on the predictions. As a practice,  those observations that have a Cook's distance greater than 4 times the mean value may be classified as an outlier. Outlier detection can be done using univariate and multivariate analysis. In univariate model, the outliers are those observations that are present outside of 1.5 * IQR, where IQR ({\em Inter Quartile Range}) is the difference between 75th and 25th quartiles. Analyzing outliers in any observations based on single variable may lead to incorrect inferences. Cook's distance generalizes the outlier analysis using multivariate approach \cite{1}. Figure (\ref{c:code-outliers}) is the code implementing Cook's distance to find the outliers from training dataset and figure (\ref{fig:outliers}) shows the scatter plot with outliers being marked as bubbles. The bigger the bubbles, the bigger outlier deviations from the mean value. We have further analyzed two key variables - {\em ground live area} and {\em garage area} that are in high correlation with the {\em sale price}. From the scatter plot shown in figure (\ref{fig:gr-liv-area-outlier}), we can see that {\em garage live area} has 4 outliers with values greater than 4,000 sq ft. We can also visualize 4 outliers in {\em garage area} scatter plot with values greater than 1,200 sq ft. as shown in figure (\ref{fig:garage-area-outlier}).  We have removed the 8 outlier rows related to these two variables from the training dataset, the corresponding code snippet shown in figure (\ref{c:code-del-outliers}).
	
	\begin{figure}[htb]		
	\begin{verbatim}	
	# python code - outlier analysis
	import statsmodels.api as sm
	from statsmodels.formula.api import ols
	
	model = ols(formula = 'SalePrice ~ 
								GrLivArea + GarageArea', data=train)
	fitted = model.fit()    	
	plot = sm.graphics.influence_plot(fitted, 
							criterion='cooks')		
	\end{verbatim}
	\caption{Code - Outlier Analysis} \label{c:code-outliers} 
	\end{figure}

	\begin{figure}[htb]
		\centering
		\includegraphics[width=.95\columnwidth]{images/outliers}	
		\caption{Graph - Outliers using Cook's distance} \label{fig:outliers} 
	\end{figure}
		
	\begin{figure}[htb]
	\centering
	\includegraphics[width=.95\columnwidth]{images/gr_liv_area_outlier}	
	\caption{Graph - Garage Live Area Outliers} \label{fig:gr-liv-area-outlier} 
	\end{figure}

	\begin{figure}[htb]
	\centering
	\includegraphics[width=.95\columnwidth]{images/garage_area_outlier}	
	\caption{Graph - Garage Area Outliers} \label{fig:garage-area-outlier} 
	\end{figure}

	\begin{figure}[htb]				
		\begin{verbatim}	
		# python code - remove outlier rows
		# fix all extreme outliers based on outlier analysis
		# 8 rows will be deleted
		train = train[train.GrLivArea <= 4000]
		train = train[train.GarageArea <= 1200]
		\end{verbatim}
		\caption{Code - Delete Outliers} \label{c:code-del-outliers} 
	\end{figure}

	\subsection{Feature Engineering}
	
	Feature engineering is a technique to analyze all the variables those influence target variable for better predictions. Part of feature engineering, we may need to create new features to make the data to be more expressive. One of the key intents, in analyzing categorical variables, is to convert them into numerical factors as most of the machine learning algorithms expect all the variables to be numeric for them to work more effectively. Some of the categorical variables are ordinal. we can use T-shirt sizes: small, medium and large as an example to explain an ordinal variable. When we convert this category variable into numeric encoding, we need to retain the fact that there is an implicit order within the values. Supposing we give ordinal encoding as - small = 1, medium = 2 and large = 3; we will satisfy the implicit order or weightage and that helps in modeling the system by elevating the importance of this implicit ordering in the values of the ordinal variable.
	
	There are a few other encoding techniques, such as one-hot, binary, polynomial and helmert to factorize categorical variables. We will use ordinal and one-hot encoding techniques for this data set. One-hot encoding converts the category variable into many binary vectors, one new numeric variable for each value in the category. Assume that we have a categorical variable called signal-light with three possible values: green, yellow and red. We will need to convert these values into numeric - green = 1, yellow = 2 and red = 3. When we apply one-hot encoding on this variable, basically we are creating three new categorical variables - signal-light-green, signal-light-yellow and signal-light-red along with the original variable - signal-light, each is pretty much a binary vector having 1s for all the corresponding values; otherwise 0s. With hot-encoding, we are basically increasing dimensions in the model. After extensive feature engineering applied on the housing dataset, we have added {\em 228} new features (variables). Figure (\ref{c:code-one-hot}) shows the python methods to factorize categorical variables and one-hot encoding techniques. 
	
	\begin{figure}[htb]	
	\begin{verbatim}	
	# python code - factorize and one-hot
	def get_one_hot(df, col_name, fill_val):
	  if fill_val is not None:
	    df[col_name].fillna(fill_val, inplace=True)
	
	  dummies = pd.get_dummies(df[col_name], prefix='_' + col_name)
	  df = df.join(dummies)
	  df = df.drop([col_name], axis=1)
	  return df
	#end def
	
	from sklearn.preprocessing import LabelEncoder
	
	def factorize(df, column, fill_na=None):
	  le = LabelEncoder()
	  if fill_na is not None:
	     df[column].fillna(fill_na, inplace=True)
	  le.fit(df[column].unique())
	  df[column] = le.transform(df[column])
	  return df
	#end def
	\end{verbatim}
	\caption{Code - factorize and one-hot encoding} \label{c:code-one-hot} 
	\end{figure}

	
	\section{Algorithms and Methodology}

	Linear regression predicts the target variable using best possible straight line fit to the set predictor variables. The possible best fit is usually the one that minimizes the root mean squared error (RMSE) between the actual and predicted data points. However, with complex problem space such as the housing prices dataset, we have lots of variables relating to the target variable in a non-linear fashion. Trivial supervised learning algorithms will not be effective to provide accurate {\em sale price} predictions. To overcome this challenge, we have applied various advanced supervised learning algorithms, such as Support Vector Machine (SVM), Random Forest, Lasso, Ridge, XGBoost and Neural Network, to predict the test data housing prices.
	
	 
	\subsection{Support Vector Machine (SVM) Algorithm}

	Support Vector Machine (SVM) algorithms can be used to solve classification and regression problems. SVM regression relies on kernel functions for modeling the data. SVM creates larger margins between categories of data so that they are linearly separable. SVM handles non-linearly separable data, mainly for regression problems, using kernel functions, such as polynomial, radial basis function (RBF) and sigmoid, to project the data onto a hyperplane. Figure (\ref{c:svm}) shows the python implementation for {\em sale price} predictions of the housing test dataset.
	
	\begin{figure}[htb]	
	\begin{verbatim}	
	# python code - SVM algorithm
	from sklearn.svm import SVR
	import matplotlib.pyplot as plt
	from sklearn.metrics import mean_squared_error
	
	train = pd.read_csv('train.csv')
	test = pd.read_csv('test.csv')
	target_vector = train['SalePrice']
	
	_svm_algo = SVR(kernel = 'rbf', C=1e3, gamma=1e-8)	
	
	_svm_algo.fit(train, target_vector)    
		
	y_train = target_vector
	y_train_pred = _svm_algo.predict(train)
	
	#root mean squared error (RMSE)
	rmse_train = np.sqrt(rmse(y_train,y_train_pred))
	
	y_test_pred = _svm_algo.predict(test)
	
	\end{verbatim}
	\caption{Code - SVM Algorithm} \label{c:svm} 
	\end{figure}
	

	\subsection{Random Forest Algorithm}

    Random Forest is an advanced machine learning algorithm for predictive analytics. Random Forest ensembles multiple decision trees to create an additive learning model from the sequence of base models created by each decision tree that worked on a sub-sample dataset. Random Forest models are suitable to handle tabular datasets with hundreds of numeric and categorical features. Along with missing values, non-linear relations between features and the target, will be handled well by random forest algorithms. With proper tuning of hyper-parameters of the random forest algorithm, it can perform well with decent accuracy in the predictions without overfitting the model. Unlike similar regression models, it does not offer feature coefficient information but it provides {\em feature ranking} functionality very nicely. Figure (\ref{c:rf}) shows the random forest algorithm details for the {\em sale price} predictions implemented using {\em sklearn} package and the figure (\ref{fig:random-feature-ranking}) shows the top 10 important features selected by random forest to model the predictions.
	
		\begin{figure}[htb]
		\begin{verbatim}		
		# python code - random forest algorithm
		from sklearn.ensemble import RandomForestRegressor
		from sklearn.metrics import accuracy_score
		from sklearn.metrics import confusion_matrix
		from sklearn.metrics import mean_squared_error
		
		train = pd.read_csv('train.csv')
		test = pd.read_csv('test.csv')
		target_vector = train['SalePrice']
		
		_algo = RandomForestRegressor(n_estimators=100, 
									oob_score=True, random_state=123456)
		
		model = _algo.fit(train, target_vector)  
		
		feat_imp = pd.Series(_algo.feature_importances_, 
							train.columns).sort_values(ascending=False)
							
		feat_imp[:10].plot(kind='bar', 
								title='Feature Ranmkingt')		
		y_train = target_vector
		y_train_pred = _algo.predict(train)
		
		#root mean squared error (RMSE)
		rmse_train = np.sqrt(rmse(y_train,y_train_pred))			
		y_test_pred = _algo.predict(test)		
		\end{verbatim}
		\caption{Code - Random Forest Algorithm} \label{c:rf} 
		\end{figure}


	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.85\columnwidth]{images/random_forest_feature_ranking}	
		\caption{Graph - Random Forest Feature Ranking} \label{fig:random-feature-ranking} 
	\end{figure}
		
	\subsection{Lasso Algorithm}
	
	Lasso is a regression model that uses shrinkage to bring data points towards the center, similar to the mean value of all the data points. Lasso stands for Least Absolute Shrinkage and Selection Operator. It is a regularized linear model with penalty term {\em lambda} to minimize the error. Parameter penalization controls overfitting the input data by shrinking variable coefficients to 0. Essentially this makes the variables no effect in the model, hence reduces the dimensions. Figure (\ref{c:lasso}) shows the lasso algorithm implementation for {\em sale price} predictions in python.
		
	\begin{figure}[htb]
	\begin{verbatim}	
	# python code - lasso algorithm
	from sklearn.linear_model import Lasso
	from sklearn.metrics import mean_squared_error
	
	train = pd.read_csv('train.csv')
	test = pd.read_csv('test.csv')
	target_vector = train['SalePrice']
	
	#found this best alpha value through cross-validation
	_best_alpha = 0.0001	
	
	_lasso_algo = Lasso(alpha = _best_alpha, 
	                    max_iter = 50000)
	
	model = _lasso_algo.fit(train, target_vector)  
		
	y_train = target_vector
	y_train_pred = _algo.predict(train)
	
	#root mean squared error (RMSE)
	rmse_train = np.sqrt(rmse(y_train,y_train_pred))
		
	y_test_pred = _lasso_algo.predict(test)	
	\end{verbatim}
	\caption{Code - Lasso Algorithm} \label{c:lasso} 
	\end{figure}
	
	\subsection{Ridge Algorithm}
	
	Ridge algorithm is very similar to lasso algorithm with the same goal. While lasso performs {\em L1 regularization}, ridge applies {\em L2 regularization} techniques in modeling the predictions. L1 regularization adds penalty to the variables equivalent to {\em absolute value of the magnitude} of the coefficients, where as L2 adds the penalty equivalent to {\em square of the magnitude} of the variable coefficients. Figure (\ref{c:ridge}) shows the python implementation of the ridge algorithm for the {\em sale price} predictions. Figures ({\ref{fig:ridge-feature-ranking-pos}}) and (\ref{fig:ridge-feature-ranking-neg}) show the top 10 positively and top 10 negatively influencing variables with {\em sale price}.
		
	\begin{figure}[htb]
	\begin{verbatim}
	# python code - ridge algorithm
	from sklearn.linear_model import Ridge
	from sklearn.metrics import mean_squared_error
	
	train = pd.read_csv('train.csv')
	test = pd.read_csv('test.csv')
	target_vector = train['SalePrice']
	
	#found this best alpha value through cross-validation
	_best_alpha = 0.00099
	
	_ridge_algo = Ridge(alpha = _best_alpha, 
	               normalize = True)		
	               	
	_ridge_algo.fit(train, target_vector)   
	
	df = {'features': train.columns.values, 
	          'Coefficients': _ridge_algo.coef_[0]}	          
	coefficients = pd.DataFrame(df)
	            .sort_values(by='Coefficients', 
	            ascending=False)
	            
	plt.figure()
	coefficients.iloc[0:10].plot(x=['features'], 
	                 kind='bar', title='Top 10 Positive Features')	                 
	plt.ylabel('Feature Coefs')	
	plt.figure()
	coefficients.iloc[-10:].plot(x=['features'], 
	                  kind='bar', title='Top 10 Negative Features')
	plt.ylabel('Feature Coefs')
	
	y_train = target_vector
	y_train_pred = _ridge_algo.predict(train)
	
	#root mean squared error (RMSE)
	rmse_train = np.sqrt(rmse(y_train,y_train_pred))	
	
	y_test_pred = _ridge_algo.predict(test)
	
	df_predict = pd.DataFrame({'Id': test['Id'], 
	'SalePrice': np.exp(y_test_pred) - 1.0})
	
	#df_predict = pd.DataFrame({'Id': id_vector, 
	'SalePrice': sale_price_vector})
	
	df_predict.to_csv('../data/kaggle_python_ridge.csv',
				 header=True, index=False)	
	\end{verbatim}
	\caption{Code - Ridge Algorithm} \label{c:ridge} 
	\end{figure}
		
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.8\columnwidth]{images/ridge_feature_ranking_pos}	
		\caption{Graph - Ridge Top 10 Positive Features} \label{fig:ridge-feature-ranking-pos} 
	\end{figure}

	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.8\columnwidth]{images/ridge_feature_ranking_neg}	
		\caption{Graph - Ridge Top 10 Negative Features} \label{fig:ridge-feature-ranking-neg} 
	\end{figure}

	\subsection{XGB Boosting Algorithm}

	XGBoost (eXtreme Gradient Boosting) is one of the Gradient Boosted Machine algorithms. It ensembles (combines) optimized model by taking trained models from all the preceding iterations. XGBoost regularizes the variables (parameters) to reduce the overfit and can work well with variables having missing values. It is empowered with built-in cross validation to reduce the boosting iterations; hence offers better performance along with parallel processing on distributed systems such as Hadoop. By tuning the XGBoost hyper parameters, we can achieve well optimized model that can make more accurate predictions. XGBoost uses {\em F-Score} to measure the importance of  variables. Table (\ref{tab:xgb-param}) explains the hyper-parameters of XGBoost algorithm and also given the python code, as shown in figure (\ref{c:xgb}), implementing for {\em sale price} predictions. Figure (\ref{fig:xgb-feature-imp}) shows the top 10 feature selection by the XGBoost.
	
	\begin{table}[htb]
		\center
		\caption{Table - XGBoost Hyper-parameters}
		\label{tab:xgb-param}		
		\begin{tabular}{l p{5cm}}
			\toprule
			Hyper-parameter & Description \\
			\midrule
			Maximum Iterations: & Number of trees in the final model. More the trees, more accuracy. \\
			
			Maximum Depth: & Depth of each individual tree to control overfitting. \\
			
			Step Size: & Shrinkage, works similar to learning rate; smaller value takes more iterations. \\
			
			Column Subsample: & Subset of the columns to use in each iteration \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{figure}[htb]
	\begin{verbatim}
	# python code - XGBoost algorithm
	import xgboost as xgb
	from xgboost import XGBClassifier
	from xgboost import plot_importance
	from sklearn.metrics import mean_squared_error
	from sklearn import cross_validation, metrics
	
	train = pd.read_csv('train.csv')
	test = pd.read_csv('test.csv')
	target_vector = train['SalePrice']
	
	_algo = np.random.seed(1234)
	
	_xgb_algo = xgb.XGBRegressor(
	colsample_bytree=0.8,
	colsample_bylevel = 0.8,
	gamma=0.01,
	learning_rate=0.05,
	max_depth=5,
	min_child_weight=1.5,
	n_estimators=6000,                                                                  
	reg_alpha=0.5,
	reg_lambda=0.5,
	subsample=0.7,
	seed=42,
	silent=1)
	
	_xgb_algo.fit(train, target_vector)   
	
	feat_imp = pd.Series(_xgb_algo.booster()
	.get_fscore())
	.sort_values(ascending=False)[0:10] 
	plot = feat_imp.plot(kind='bar', 
	title='Top 10 Feature Importances')
	
	y_train = target_vector
	y_train_pred = _xgb_algo.predict(train)
	
	#root mean squared error (RMSE)
	rmse_train = np.sqrt(rmse(y_train,y_train_pred))
	
	y_test_pred = _xgb_algo.predict(test)
	
	\end{verbatim}
	\caption{Code - XGBoost Algorithm} \label{c:xgb} 
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.75\columnwidth]{images/xgboost_feature_importance}	
		\caption{Graph - XGBoost Feature Importance} \label{fig:xgb-feature-imp} 
	\end{figure}


	\subsection{Neural Network Algorithm}
	Neural Network is, a {\em directed graph}, organized by layers and layers are created by number of interconnected neurons (or nodes). Every neuron in a layer is connected with all the neurons from previous layer; there will be no interaction of neurons within a layer. The performance of a Neural Network is measured using {\em cost or error function} and the dependent input {\em weight} variables. {\em Forward-propagation} and {\em back-propagation} are two techniques, neural network uses repeatedly until all the input variables are adjusted or calibrated to predict accurate output. During, forward-propagation, information moves in forward direction and passes through all the layers by applying certain weights to the input parameters. {\em Back-propagation} method minimizes the error in the {\em weights} by applying an algorithm called {\em gradient descent} at each iteration step. We have used {\em TensorFlow} python library to predict the {\em sale price} of housing dataset using simple feed-forward neural network. TensorFlow uses {\em tensors}, special multi-dimensional arrays to store the datasets for easier linear algebra and vector calculus operations.
			
	\subsection{Model Ensembling}
	We can create a robust predictive model with better accuracy by merging two or more machine learning algorithms. This technique is called {\em model ensembling}. Ensembled algorithms may be similar in functionality or may entirely be different from each other. Individual algorithms may not perform great but by ensembling them, the overall system can offer much better performance and accuracy. Variations in the predicting logic in each of these individual algorithms will bring unbiasedness into the unified model. {\em Bagging}, {\em boosting} and {\em stacking} are popular ensembling techniques. Many of the advanced machine learning algorithms use ensembled approaches to achieve accurate classifications or predictions. Random Forest uses bagging, XGBoost uses boosting and Neural Network applies stacking ensembling techniques. For the kaggle submission, we have created an ensembled model by averaging {\em Sale Price} of the top 3 performing ensembled algorithms - XGBoost, Lasso and Neural Network. As predicted, ensembled model has scored better compared to the individual algorithms. By applying advanced machine learning algorithms, we have placed our scores within top 15\% of the competition. Table (\ref{tab:kaggle}) displays each algorithm and the {\em root mean squared error} (RMSE) along with the {\em kaggle score}.
					
	\begin{table}[htb]
		\caption{Table - Kaggle Submissions}
		\label{tab:kaggle}		
		\begin{tabular}{lrr}
			\toprule
			Algorithm & RMSE & Kaggle Score \\
			\midrule
		    SVM &  0.2069 & 0.23967 \\
		    Random Forest &  0.0519 & 0.14607 \\
			Ridge &  0.0988 & 0.13687 \\
			XGBoost &  0.0432 & 0.13018 \\					
			Lasso &  0.1015 & 0.12860 \\
			Neural Network &  0.20 & 0.12510 \\
			Ensemble &   & 0.12011 \\
			\bottomrule
		\end{tabular}
	\end{table}

   \section{Development Environment}
    
    \subsection{OS and Programming Language}
    We have used {\em Ubuntu 16.4} Operating System that runs in Windows 10 through Oracle Virtual Box 5.2. Python 2.7  has been used as the programming language for this project. Data visualizations are done using {\em seaborn} and {\em matplotlib} packages. Most of the algorithms implemented in this project are using {\em sklearn} package. For the neural network algorithm, we have used {\em tensorflow} package as it offers simple programming interface to the complex processing needed by the algorithm. Our code is placed in gitbub repository at {\em  git@github.com:bigdata-i523/hid306.git}. 
    
    \subsection{Project Folder Structure}
    Project is organized in three folders - code, data and images. Code folder has all the python code files. Data folder contains the {\em house pricing} sample datasets provided by Kaggle website for the housing prices competition that we used for the exploratory analysis and {\em sale price} predictions. We also stored all the {\em sale price} prediction output files from various algorithms in the data folder. Images folder contains all the data visualization files that we have created during the analysis and in processing the algorithms. We wanted to create interactive and sharable code files that contain not only the python code but also corresponding explanation along with data visualizations. Jupyter Notebook application is ideal for such facilitation with python code components. Using Jupyter Notebook, it would be easy to share live code with the reviewers. Such environment allows to explore the code-base easily along with the interactive code execution and visualize all the corresponding exploratory analysis results with the graphs.
    
    \subsection{Project Files}
    We have a total of 11 Jupyter Notebook driven python code files. First 4 files are focused on doing the exploratory data analysis and the next 6 files are meant for six supervised machine learning algorithms - SVM, Random Forest, Ridge, Lasso, Neural Network and XGBoost. Last code file is dedicated for ensembling the top 3 algorithms with best predictions of housing sale prices in the test dataset. We have named them in a sequence as there is an implicit order in the execution of these files. We wanted to do the data analysis first before running the predictive algorithms. 
        
    \subsection{List of Code Files}    
    Following is the list of code files:
    \begin{enumerate}
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/1.1_exploratory_analysis_numerical.ipynb}{1.1\_exploratory\_analysis\_numerical.ipynb} - To load datasets and analyze all numerical variables
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/1.2_exploratory_analysis_categorical.ipynb}{1.2\_exploratory\_analysis\_categorical.ipynb} - To analyze categorical variables in the dataset
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/1.3_outlier_and_skewed_data_analysis.ipynb}{1.3\_outlier\_and\_skewed\_data\_analysis.ipynb} - Handles outlier and skewed data analysis
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/1.4_feature_engineering.ipynb}{1.4\_feature\_engineering.ipynb} - All the feature engineering is done in this file
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/2.1_algorithm_svm.ipynb}{2.1\_algorithm\_svm.ipynb} - Implementation of SVM algorithm
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/2.2_algorithm_random_forest.ipynb}{2.2\_algorithm\_random\_forest.ipynb} - Implementation of Random Forest algorithm
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/2.3_algorithm_ridge.ipynb}{2.3\_algorithm\_ridge.ipynb} - Implementation of Ridge algorithm
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/2.4_algorithm_lasso.ipynb}{2.4\_algorithm\_lasso.ipynb} - Implementation of Lasso algorithm
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/2.5_algorithm_neural_network_tf.ipynb}{2.5\_algorithm\_neural\_network\_tf.ipynb} - Implementation of Neural Network algorithm
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/2.6_algorithm_xgboost.ipynb}{2.6\_algorithm\_xgboost.ipynb} - Implementation of XGBoost algorithm
    	
    	\item Code: \href{https://github.com/bigdata-i523/hid306/blob/master/project/code/3_ensemble_kaggle_submission.ipynb}{3\_ensemble\_kaggle\_submission.ipynb} - Implementation of Ensembled algorithm
    \end{enumerate}    
    
     \subsection{List of Data Files}    
     Following is the list of data files:
    \begin{enumerate}        
    	\item Input data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/train.csv}{train.csv} - Sample training dataset with housing attributes along with the sale price
    	\item input data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/test.csv}{test.csv} - Sample testing dataset similar to training dataset without the sale price
    	
       	\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_svm.csv}{kaggle\_python\_svm.csv} - Predicted Housing Sale Prices from SVM algorithm
       	
		\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_random_forest.csv}{kaggle\_python\_random\_forest.csv} - Predicted Housing Sale Prices from Random Forest algorithm
		
		\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_ridge.csv}{kaggle\_python\_ridge.csv} - Predicted Housing Sale Prices from Ridge algorithm
		
		\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_xgboost.csv}{kaggle\_python\_xgboost.csv} - Predicted Housing Sale Prices from XGBoost algorithm
		
		\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_lasso.csv}{kaggle\_python\_lasso.csv} - Predicted Housing Sale Prices from Lasso algorithm
		
		\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_neural_network.csv}{kaggle\_python\_neural\_network.csv} - Predicted Housing Sale Prices from Neural Network algorithm
		
		\item Output data file: \href{https://github.com/bigdata-i523/hid306/blob/master/project/data/kaggle_python_ensemble.csv}{kaggle\_python\_ensemble.csv} - Predicted Housing Sale Prices from Ensembled algorithm    	
	\end{enumerate}    
   
   
	\section{Conclusion}
	
	Generally, ensemble models performs better compared to individual algorithms. However, there are a few factors that influence accuracy and performance of the algorithms, such as handcrafted feature engineering, proper cost function with regularized input to address non-linearities in the training datasets and tuning hyper-parameters of the algorithms. While Deep Learning Neural Networks are good for image processing, K-Nearest Neighbor algorithms can handle unsupervised datasets with less complexity. Domain knowledge and algorithm selection play vital role in getting accurate predictions. XGBoost, Random Forest, Lasso and Neural Networks are advanced machine learning algorithms dominating in the data science competitions for classification and regression related tasks. With ensembling and iterative learning techniques, they can scale well and offer better predictions for huge datasets having large number of features. 
		
	\appendix
	
	%Appendix A
	\section{Sample Dataset File Details}
	The training and testing sample datasets provided by Kaggle contain the same variables explaining the housing real estate aspects. Training dataset contains the sale price information whereas the testing dataset does not the sale price as that is the target variable we need to predict using supervised machine learning algorithm. Following are the list of variables describing the housing real estate domain. Good understanding of the domain is needed for better exploratory data analysis and to apply the matching machine learning algorithms to the problem space.
	
	\begin{itemize}
	\item Id: Row Id
	\item SalePrice: Sale price of the house in dollars. This is the target variable to predict.
	\item MSSubClass: The building class
	\item MSZoning: The general zoning classification
	\item LotFrontage: Linear feet of street connected to property
	\item LotArea: Lot size in square feet
	\item Street: Type of road access
	\item Alley: Type of alley access
	\item LotShape: General shape of property
	\item LandContour: Flatness of the property
	\item Utilities: Type of utilities available
	\item LotConfig: Lot configuration
	\item LandSlope: Slope of property
	\item Neighborhood: Physical locations within Ames city limits
	\item Condition1: Proximity to main road or railroad
	\item Condition2: Proximity to main road or railroad (if a second is present)
	\item BldgType: Type of dwelling
	\item HouseStyle: Style of dwelling
	\item OverallQual: Overall material and finish quality
	\item OverallCond: Overall condition rating
	\item YearBuilt: Original construction date
	\item YearRemodAdd: Remodel date
	\item RoofStyle: Type of roof
	\item RoofMatl: Roof material
	\item Exterior1st: Exterior covering on house
	\item Exterior2nd: Exterior covering on house (if more than one material)
	\item MasVnrType: Masonry veneer type
	\item MasVnrArea: Masonry veneer area in square feet
	\item ExterQual: Exterior material quality
	\item ExterCond: Present condition of the material on the exterior
	\item Foundation: Type of foundation
	\item BsmtQual: Height of the basement
	\item BsmtCond: General condition of the basement
	\item BsmtExposure: Walkout or garden level basement walls
	\item BsmtFinType1: Quality of basement finished area
	\item BsmtFinSF1: Type 1 finished square feet
	\item BsmtFinType2: Quality of second finished area (if present)
	\item BsmtFinSF2: Type 2 finished square feet
	\item BsmtUnfSF: Unfinished square feet of basement area
	\item TotalBsmtSF: Total square feet of basement area
	\item Heating: Type of heating
	\item HeatingQC: Heating quality and condition
	\item CentralAir: Central air conditioning Electrical: Electrical system
	\item 1stFlrSF: First Floor square feet
	\item 2ndFlrSF: Second floor square feet
	\item LowQualFinSF: Low quality finished square feet (all floors)
	\item GrLivArea: Above grade (ground) living area square feet
	\item BsmtFullBath: Basement full bathrooms
	\item BsmtHalfBath: Basement half bathrooms
	\item FullBath: Full bathrooms above grade
	\item HalfBath: Half baths above grade
	\item Bedroom: Number of bedrooms above basement level
	\item Kitchen: Number of kitchens
	\item KitchenQual: Kitchen quality
	\item TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
	\item Functional: Home functionality rating
	\item Fireplaces: Number of fireplaces
	\item FireplaceQu: Fireplace quality
	\item GarageType: Garage location
	\item GarageYrBlt: Year garage was built
	\item GarageFinish: Interior finish of the garage
	\item GarageCars: Size of garage in car capacity
	\item GarageArea: Size of garage in square feet
	\item GarageQual: Garage quality
	\item GarageCond: Garage condition
	\item PavedDrive: Paved driveway
	\item WoodDeckSF: Wood deck area in square feet
	\item OpenPorchSF: Open porch area in square feet
	\item EnclosedPorch: Enclosed porch area in square feet
	\item 3SsnPorch: Three season porch area in square feet
	\item ScreenPorch: Screen porch area in square feet
	\item PoolArea: Pool area in square feet
	\item PoolQC: Pool quality
	\item Fence: Fence quality
	\item MiscFeature: Miscellaneous feature not covered in other categories
	\item MiscVal: Dollar Value of miscellaneous feature
	\item MoSold: Month Sold
	\item YrSold: Year Sold
	\item SaleType: Type of sale
	\item SaleCondition: Condition of sale
    \end{itemize}

	\nocite{*}
	
	\begin{acks}	
			The authors would like to thank Dr. Gregor von Laszewski and the Teaching Assistants for their support and great suggestions. Authors would also want to thank Kaggle Website and the developers for their valuable information, ideas and contributions.		
	\end{acks}


	\bibliographystyle{ACM-Reference-Format}
	\bibliography{report} 	

	
\end{document}
